# DeltaF1-Hybrid-Active-Learning

This project implements a hybrid training strategy for pool-based active learning. The strategy adaptively switches between retraining and fine-tuning a model based on the change in macro-F1 score (Î”F1) across rounds.

## ğŸŒŸ Goal

To reduce training cost in active learning without sacrificing performance, we propose a hybrid training strategy that switches from retraining to fine-tuning once Î”F1 falls below a small threshold Îµ for k consecutive rounds.

## ğŸ” Datasets

We experiment on three text classification benchmarks:

- [AG News](https://huggingface.co/datasets/ag_news) â€“ Topic classification (balanced)
- [IMDb Reviews](https://huggingface.co/datasets/imdb) â€“ Sentiment analysis (binary)
- [Jigsaw Toxic Comments](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) â€“ Multi-label toxicity classification (imbalanced)

## ğŸ§  Models

We use `DistilBERT` as the base encoder, fine-tuned separately per dataset with consistent hyperparameters.

## ğŸ” Training Strategies

* `retrain`: Full retraining from scratch on all labeled data every round.
* `fine-tune`: Warm-start from the previous round and continue training on all labeled data.
* `new-only`: Fine-tune only on the newly labeled batch each round (no replay of past data).
* `hybrid`: Start with retraining, then switch to fine-tuning (on all labeled data) when Î”F1 < Îµ for $k$ consecutive rounds.

> All strategies use the same model architecture and hyperparameters to ensure fair comparison.

## ğŸ“Š Evaluation

- **Metric**: Macro-F1 score (primary)
- **Switch trigger**: Î”F1 < Îµ  for \( k \) rounds  
- **Other metrics**: training time, label efficiency  
- Results include plots of learning curves, Î”F1 trends, and switch timing.

## ğŸ›  Repo Structure
hybrid_active_learning/
â”œâ”€â”€ data/                 # Raw and preprocessed datasets (AG News, IMDb, Jigsaw)
â”œâ”€â”€ media/
â”œâ”€â”€ scripts/              # All source code
â”‚   â”œâ”€â”€ utils.py              # Shared helpers: seeding, timing, plotting, etc.
â”‚   â”œâ”€â”€ 
â”‚   â””â”€â”€ 
â”œâ”€â”€ models/              # saved model checkpoints
â”œâ”€â”€ results/             # Metrics logs, Î”F1 values, plots
â”œâ”€â”€ README.md            # Project description and setup guide
â”œâ”€â”€ requirements.txt     # List of dependencies
â”œâ”€â”€ eda_preprocessing/
â”‚   â”œâ”€â”€ dataset_eda.ipynb  # Initial exploration of AG News, IMDb, Jigsaw
â”‚   â”œâ”€â”€ split_saver.ipynb # Creating and saving train/val/test splits
â”‚   â”œâ”€â”€ 
â”‚   â””â”€â”€ 
â””â”€â”€ config.json          # Experiment config: Îµ, k, batch size, etc.



## âš™ï¸ Requirements

- Python 3.10
- PyTorch
- scikit-learn
- datasets
- matplotlib, pandas, tqdm, numpy
- 

To install dependencies:

```bash
pip install -r requirements.txt
