{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:43.408244Z",
     "start_time": "2025-06-22T18:26:33.727626Z"
    }
   },
   "source": [
    "import optuna\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import data_loader\n",
    "import tqdm.auto as tqdm\n",
    "with open(\"../../config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:43.704315Z",
     "start_time": "2025-06-22T18:26:43.691606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute macro-F1 score for model evaluation.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple of (logits, true labels).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with macro-F1 score.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }"
   ],
   "id": "d20c806cca1a0062",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:43.750192Z",
     "start_time": "2025-06-22T18:26:43.737369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def model_init(model_name, num_labels):\n",
    "    \"\"\"\n",
    "    Initialize a model with a given architecture and number of labels.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): HuggingFace model identifier.\n",
    "        num_labels (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        function: A function that returns a new model instance.\n",
    "    \"\"\"\n",
    "    return lambda: AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ],
   "id": "9d67b9927517d1db",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:43.796480Z",
     "start_time": "2025-06-22T18:26:43.767409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search_best_hparams(train_dataset, val_dataset, model_name=\"distilbert-base-uncased\",\n",
    "                        num_labels=2, n_trials=50, output_dir=\"optuna_search\"):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using Optuna.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (Dataset): HuggingFace Dataset for training.\n",
    "        val_dataset (Dataset): HuggingFace Dataset for validation.\n",
    "        model_name (str): Pretrained model name (default: DistilBERT).\n",
    "        num_labels (int): Number of classification labels.\n",
    "        n_trials (int): Number of Optuna trials.\n",
    "        output_dir (str): Folder to save training outputs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameters found by Optuna.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess(examples):\n",
    "        \"\"\"Tokenize the input text.\"\"\"\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "    tokenized_val = val_dataset.map(preprocess, batched=True)\n",
    "\n",
    "    def objective(trial):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "\n",
    "        Defines the search space and returns the evaluation metric to maximize.\n",
    "        \"\"\"\n",
    "        import random\n",
    "        def set_seed(seed=42):\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        set_seed(42)\n",
    "\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=os.path.join(output_dir, f\"trial_{trial.number}\"),\n",
    "            save_strategy=\"no\",\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "            per_device_train_batch_size=trial.suggest_categorical(\"batch_size\", [8, 16, 32]),\n",
    "            num_train_epochs=trial.suggest_int(\"epochs\", 2, 5),\n",
    "            logging_steps=10,\n",
    "            eval_steps=50,\n",
    "            disable_tqdm=True,\n",
    "            report_to=\"none\",\n",
    "            weight_decay=trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "            warmup_steps=trial.suggest_int(\"warmup_steps\", 0, 500),\n",
    "        )\n",
    "\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_init=model_init(model_name, num_labels),\n",
    "            args=args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        return metrics[\"eval_macro_f1\"]\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials,show_progress_bar=True,timeout=1800)\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "    optuna.visualization.plot_param_importances(study)\n",
    "\n",
    "    return study.best_params\n"
   ],
   "id": "df41ff9239a09dbb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## chooing hyperparameters",
   "id": "d1262bd723db970b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:43.841348Z",
     "start_time": "2025-06-22T18:26:43.829370Z"
    }
   },
   "cell_type": "code",
   "source": "initial_pool_size = config[\"initial_pool_size\"]",
   "id": "19e0f6e0f707c313",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agnews",
   "id": "6f3fe62245862eed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:47.087954Z",
     "start_time": "2025-06-22T18:26:43.860284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load all data\n",
    "train_dataset, val_dataset, test_dataset = data_loader.load_agnews()\n",
    "\n",
    "\n",
    "# Sample initial pool stratified by label\n",
    "initial_pool, _ = train_test_split(\n",
    "    train_dataset,\n",
    "    train_size=initial_pool_size,\n",
    "    stratify=train_dataset[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(initial_pool.shape)\n",
    "print(initial_pool[\"label\"].value_counts())\n"
   ],
   "id": "b4187510a92292f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n",
      "1    50\n",
      "0    50\n",
      "2    50\n",
      "3    50\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:26:47.197185Z",
     "start_time": "2025-06-22T18:26:47.136917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run hyperparameter search\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(initial_pool)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "val_dataset=val_dataset.shuffle(seed=42).select(range(500))"
   ],
   "id": "54a9a59481a2da36",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T18:44:36.091217Z",
     "start_time": "2025-06-22T18:26:47.231875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agnws = search_best_hparams(train_dataset, val_dataset, model_name=\"distilbert-base-uncased\",num_labels=4 )\n",
    "\n",
    "# Save the best parameters\n",
    "# import json, os\n",
    "# os.makedirs(\"optuna_search\", exist_ok=True)\n",
    "# with open(os.path.join(\"optuna_search\", \"best_params_agnews.json\"), \"w\") as f:\n",
    "#     json.dump(agnws, f, indent=2)\n"
   ],
   "id": "d41a4b45a35d90b2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 3376.25 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6676.89 examples/s]\n",
      "[I 2025-06-22 21:26:48,161] A new study created in memory with name: no-name-1818dafa-af2f-45d7-83d3-315a54077205\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3791, 'grad_norm': 2.0598487854003906, 'learning_rate': 3.161253558552717e-07, 'epoch': 0.4}\n",
      "{'loss': 1.3849, 'grad_norm': 2.109316349029541, 'learning_rate': 6.673757512500181e-07, 'epoch': 0.8}\n",
      "{'loss': 1.3747, 'grad_norm': 1.9343794584274292, 'learning_rate': 1.0186261466447645e-06, 'epoch': 1.2}\n",
      "{'loss': 1.3758, 'grad_norm': 1.8497302532196045, 'learning_rate': 1.369876542039511e-06, 'epoch': 1.6}\n",
      "{'loss': 1.3784, 'grad_norm': 2.253995180130005, 'learning_rate': 1.7211269374342572e-06, 'epoch': 2.0}\n",
      "{'loss': 1.3583, 'grad_norm': 2.4570188522338867, 'learning_rate': 2.0723773328290036e-06, 'epoch': 2.4}\n",
      "{'loss': 1.3645, 'grad_norm': 3.0594639778137207, 'learning_rate': 2.42362772822375e-06, 'epoch': 2.8}\n",
      "{'train_runtime': 439.3293, 'train_samples_per_second': 1.366, 'train_steps_per_second': 0.171, 'train_loss': 1.3728361320495606, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.357035:   2%|▏         | 1/50 [08:20<6:48:58, 500.80s/it, 500.79/1800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3529845476150513, 'eval_macro_f1': 0.35703489617157, 'eval_runtime': 59.6277, 'eval_samples_per_second': 8.385, 'eval_steps_per_second': 1.057, 'epoch': 3.0}\n",
      "[I 2025-06-22 21:35:08,949] Trial 0 finished with value: 0.35703489617157 and parameters: {'learning_rate': 1.015113642690817e-05, 'batch_size': 8, 'epochs': 3, 'weight_decay': 0.0647265149192234, 'warmup_steps': 289}. Best is trial 0 with value: 0.35703489617157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3845, 'grad_norm': 1.3488456010818481, 'learning_rate': 2.977488257640279e-07, 'epoch': 1.4285714285714286}\n",
      "{'loss': 1.3833, 'grad_norm': 1.2603083848953247, 'learning_rate': 6.285808543907256e-07, 'epoch': 2.857142857142857}\n",
      "{'train_runtime': 365.3342, 'train_samples_per_second': 1.642, 'train_steps_per_second': 0.057, 'train_loss': 1.3826742001942225, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.357035:   4%|▍         | 2/50 [15:38<6:11:06, 463.89s/it, 938.85/1800 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3814243078231812, 'eval_macro_f1': 0.12146293962653223, 'eval_runtime': 71.4177, 'eval_samples_per_second': 7.001, 'eval_steps_per_second': 0.882, 'epoch': 3.0}\n",
      "[I 2025-06-22 21:42:27,000] Trial 1 finished with value: 0.12146293962653223 and parameters: {'learning_rate': 1.544985573686678e-05, 'batch_size': 32, 'epochs': 3, 'weight_decay': 0.06251004940582387, 'warmup_steps': 467}. Best is trial 0 with value: 0.35703489617157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3789, 'grad_norm': 2.059345006942749, 'learning_rate': 7.59813038259011e-07, 'epoch': 0.4}\n",
      "{'loss': 1.3839, 'grad_norm': 2.111571788787842, 'learning_rate': 1.6040497474356898e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.357035:   4%|▍         | 2/50 [17:44<7:05:57, 532.45s/it, 938.85/1800 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-06-22 21:44:33,033] Trial 2 failed with parameters: {'learning_rate': 1.6462615828945238e-05, 'batch_size': 8, 'epochs': 2, 'weight_decay': 0.03999596550792106, 'warmup_steps': 195} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Nagham Omar\\AppData\\Local\\Temp\\ipykernel_4328\\378710210.py\", line 67, in objective\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\transformers\\trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\transformers\\trainer.py\", line 2555, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\transformers\\trainer.py\", line 3791, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\accelerate\\accelerator.py\", line 2553, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\torch\\_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\Users\\Nagham Omar\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-22 21:44:33,065] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m agnws \u001B[38;5;241m=\u001B[39m \u001B[43msearch_best_hparams\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdistilbert-base-uncased\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Save the best parameters\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# import json, os\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# os.makedirs(\"optuna_search\", exist_ok=True)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# with open(os.path.join(\"optuna_search\", \"best_params_agnews.json\"), \"w\") as f:\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#     json.dump(agnws, f, indent=2)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[4], line 72\u001B[0m, in \u001B[0;36msearch_best_hparams\u001B[1;34m(train_dataset, val_dataset, model_name, num_labels, n_trials, output_dir)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_macro_f1\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     71\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 72\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1800\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m optuna\u001B[38;5;241m.\u001B[39mvisualization\u001B[38;5;241m.\u001B[39mplot_optimization_history(study)\n\u001B[0;32m     74\u001B[0m optuna\u001B[38;5;241m.\u001B[39mvisualization\u001B[38;5;241m.\u001B[39mplot_param_importances(study)\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\optuna\\study\\study.py:489\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    389\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    397\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    398\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    399\u001B[0m \n\u001B[0;32m    400\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    487\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 489\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:64\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 64\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:161\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 161\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:253\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    246\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    249\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    252\u001B[0m ):\n\u001B[1;32m--> 253\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 201\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    204\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[4], line 67\u001B[0m, in \u001B[0;36msearch_best_hparams.<locals>.objective\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m     44\u001B[0m args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m     45\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrial_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrial\u001B[38;5;241m.\u001B[39mnumber\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m     46\u001B[0m     save_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     55\u001B[0m     warmup_steps\u001B[38;5;241m=\u001B[39mtrial\u001B[38;5;241m.\u001B[39msuggest_int(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarmup_steps\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m500\u001B[39m),\n\u001B[0;32m     56\u001B[0m )\n\u001B[0;32m     59\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     60\u001B[0m     model_init\u001B[38;5;241m=\u001B[39mmodel_init(model_name, num_labels),\n\u001B[0;32m     61\u001B[0m     args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[0;32m     65\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m metrics \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_macro_f1\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\transformers\\trainer.py:2240\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2238\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2239\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2241\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2245\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\transformers\\trainer.py:2555\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2548\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2549\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2550\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2551\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2552\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2553\u001B[0m )\n\u001B[0;32m   2554\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2555\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2558\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2559\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2560\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2561\u001B[0m ):\n\u001B[0;32m   2562\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2563\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\transformers\\trainer.py:3791\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3788\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m==\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED:\n\u001B[0;32m   3789\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscale_wrt_gas\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 3791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mbackward(loss, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   3793\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:2553\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2551\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[0;32m   2552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2553\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\torch\\_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    625\u001B[0m     )\n\u001B[1;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\deltaf1-hybrid-active-learning\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    824\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    825\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12a963754b28efe4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
