{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-02T09:36:37.190372Z",
     "start_time": "2025-09-02T09:36:33.921575Z"
    }
   },
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from adaptive_al_v2.active_learning import ActiveLearning, ExperimentConfig\n",
    "\n",
    "import logging\n",
    "# Comment it out if you dont want to see info logs\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saba\\miniconda3\\envs\\ds-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:36:37.209320Z",
     "start_time": "2025-09-02T09:36:37.193358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1ee195758ca57c80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:37:24.864314Z",
     "start_time": "2025-09-02T09:37:24.860315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = ExperimentConfig(\n",
    "    seed=42,\n",
    "    total_rounds=5,\n",
    "    experiment_name=\"dummy_test_pipeline\",\n",
    "    save_dir=Path(\"./experiments\"),\n",
    "\n",
    "    # Pool settings\n",
    "    initial_pool_size=100,\n",
    "    acquisition_batch_size=256,\n",
    "\n",
    "    # Model\n",
    "    model_name_or_path=\"distilbert-base-uncased\",\n",
    "    num_labels=4, # TODO: maybe make it figure it out on its own based on dataset\n",
    "    tokenizer_kwargs={\n",
    "        \"max_length\": 128,\n",
    "        \"padding\": \"max_length\",\n",
    "        \"truncation\": True,\n",
    "        \"add_special_tokens\": True,\n",
    "        \"return_tensors\": \"pt\"\n",
    "    },\n",
    "\n",
    "    # Dataset names (for reference)\n",
    "    data=\"agnews\",\n",
    "\n",
    "    # Strategy\n",
    "    strategy_class=\"FineTuneStrategy\",\n",
    "    strategy_kwargs={}, # The base params are passed internally, only strategy specific params needed here\n",
    "\n",
    "    optimizer_class = \"Adam\",\n",
    "    optimizer_kwargs = {\"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "\n",
    "    criterion_class = \"CrossEntropyLoss\",\n",
    "    criterion_kwargs = {},\n",
    "\n",
    "    scheduler_class = \"StepLR\",\n",
    "    scheduler_kwargs = {\"step_size\": 10, \"gamma\": 0.1},\n",
    "\n",
    "    # Sampler\n",
    "    # sampler_class=\"RandomSampler\",\n",
    "    # sampler_kwargs={\"seed\": 42},\n",
    "    sampler_class=\"EntropySampler\",\n",
    "    sampler_kwargs={\"show_progress\": True},\n",
    "\n",
    "    # Training\n",
    "    device=device,\n",
    "    epochs=3,\n",
    "    batch_size=64\n",
    ")"
   ],
   "id": "cb57004dfbea90ca",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:37:26.727384Z",
     "start_time": "2025-09-02T09:37:25.600258Z"
    }
   },
   "cell_type": "code",
   "source": "al = ActiveLearning(cfg)",
   "id": "c15c6ebae79dba52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer and model from 'distilbert-base-uncased'...\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:root:Train size: 108000, Validation size: 12000, Test size: 7600\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Just checking if it actually works",
   "id": "8af4dc97750a51d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:37:34.419692Z",
     "start_time": "2025-09-02T09:37:34.415839Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Initial pool stats: {al.pool.get_pool_stats()}\")",
   "id": "8485b7bb611031dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial pool stats: {'labeled_count': 100, 'unlabeled_count': 107900, 'total_count': 108000}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:38:36.246419Z",
     "start_time": "2025-09-02T09:37:34.890096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "round_stats = al.train_one_round(new_indices=None)\n",
    "print(f\"Round 1 completed. Val F1: {round_stats['f1_score']:.4f}, Training Time: {round_stats['training_time']:.2f}s\")"
   ],
   "id": "24d4cec80a9aa879",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "--- Round 1\n",
      "INFO:root:Round 1 complete. Val Stats: Loss=1.1085674252281799, F1=0.33795729128853935, Time=3.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 completed. Val F1: 0.3380, Training Time: 3.03s\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:39:21.128743Z",
     "start_time": "2025-09-02T09:38:36.274279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_indices = al.sample_next_batch()\n",
    "print(f\"Sampled {len(new_indices)} new indices: {new_indices[:5]} ...\")"
   ],
   "id": "d4114fb6e0f50bc2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m new_indices = al.sample_next_batch()\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSampled \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(new_indices)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m new indices: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnew_indices[:\u001B[32m5\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m ...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\projects\\deltaf1-hybrid-active-learning\\adaptive_al_v2\\active_learning.py:224\u001B[39m, in \u001B[36mActiveLearning.sample_next_batch\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    221\u001B[39m     logging.warning(\u001B[33m\"\u001B[39m\u001B[33mNo unlabeled data remaining!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    222\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m selected_indices = \u001B[38;5;28mself\u001B[39m.sampler.select(\u001B[38;5;28mself\u001B[39m.pool, \u001B[38;5;28mself\u001B[39m.cfg.acquisition_batch_size)\n\u001B[32m    225\u001B[39m \u001B[38;5;66;03m# DO NOT ADD THEM TO THE POOL RIGHT AWAY DUMBAHH\u001B[39;00m\n\u001B[32m    226\u001B[39m \u001B[38;5;66;03m# Letting the strategy decide what to do with it\u001B[39;00m\n\u001B[32m    227\u001B[39m \u001B[38;5;66;03m# self.pool.add_labeled_samples(selected_indices)\u001B[39;00m\n\u001B[32m    229\u001B[39m logging.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSampled \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(selected_indices)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m new samples using \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m.sampler).\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\projects\\deltaf1-hybrid-active-learning\\adaptive_al_v2\\samplers\\entropy_sampler.py:58\u001B[39m, in \u001B[36mEntropySampler.select\u001B[39m\u001B[34m(self, pool, num_samples)\u001B[39m\n\u001B[32m     55\u001B[39m         log_probs = torch.log(probs)\n\u001B[32m     56\u001B[39m         entropy_vals = -torch.sum(probs * log_probs, dim=\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m         all_entropies.append(entropy_vals.cpu())\n\u001B[32m     60\u001B[39m all_entropies = np.concatenate([t.numpy() \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m all_entropies])\n\u001B[32m     62\u001B[39m \u001B[38;5;66;03m# Ensure we don't try to select more than we have\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T09:39:21.148964900Z",
     "start_time": "2025-09-02T09:30:20.754205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "round_stats = al.train_one_round(new_indices=new_indices) [56133, 101996, 103133, 27327, 17750]\n",
    "print(f\"Round 2 completed. Val F1: {round_stats['f1_score']:.4f}, Training Time: {round_stats['training_time']:.2f}s\")"
   ],
   "id": "f5d16592aea125e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "--- Round 2\n",
      "INFO:root:Training with 256 new samples\n",
      "INFO:root:Round 2 complete. Val Stats: Loss=1.0653315877660792, F1=0.38525340550809045, Time=10.05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2 completed. Val F1: 0.3853, Training Time: 10.05s\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:05:08.921047Z",
     "start_time": "2025-08-29T13:59:57.382325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_additional_rounds = 3\n",
    "for r in range(num_additional_rounds):\n",
    "    print(f\"\\n--- Round {al.current_round + 1}\")\n",
    "\n",
    "    new_indices = al.sample_next_batch()\n",
    "    if not new_indices:\n",
    "        print(\"No more unlabeled data available!\")\n",
    "        break\n",
    "\n",
    "    round_stats = al.train_one_round(new_indices=new_indices)\n",
    "    print(f\"Val F1: {round_stats['f1_score']:.4f}, Training Time: {round_stats['training_time']:.2f}s\")\n",
    "    print(f\"Pool Stats: {round_stats['pool_stats']}\")"
   ],
   "id": "46d363e465beebb7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 6\n",
      "INFO:root:Training with 256 new samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Round 6 complete. Val Stats: Loss=1.325510044047173, F1=0.19949427609107603, Time=40.00s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 7\n",
      "INFO:root:Training with 256 new samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.1995, Training Time: 40.00s\n",
      "Pool Stats: {'labeled_count': 1380, 'unlabeled_count': 106620, 'total_count': 108000}\n",
      "\n",
      "--- Round 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Round 7 complete. Val Stats: Loss=1.2804185843214075, F1=0.24051267661223574, Time=45.39s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 8\n",
      "INFO:root:Training with 256 new samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2405, Training Time: 45.39s\n",
      "Pool Stats: {'labeled_count': 1636, 'unlabeled_count': 106364, 'total_count': 108000}\n",
      "\n",
      "--- Round 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Round 8 complete. Val Stats: Loss=1.2802721496592178, F1=0.24539144035730043, Time=53.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1: 0.2454, Training Time: 53.01s\n",
      "Pool Stats: {'labeled_count': 1892, 'unlabeled_count': 106108, 'total_count': 108000}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FULL PIPELINE HERE ! ! !",
   "id": "5989786f4babe408"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:11:53.857240Z",
     "start_time": "2025-08-29T14:05:08.942838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_metrics = al.run_full_pipeline()\n",
    "print(f\"Final Test Metrics: F1={final_metrics['f1_score']:.4f}, Accuracy={final_metrics['accuracy']:.4f}, Loss={final_metrics['loss']:.4f}\")"
   ],
   "id": "b2976e59fc9195f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading tokenizer and model from 'distilbert-base-uncased'...\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:root:Train size: 108000, Validation size: 12000, Test size: 7600\n",
      "WARNING:root:No seed was provided for RandomSampler.\n",
      "INFO:root:Running 5 rounds.\n",
      "INFO:root:\n",
      "--- Round 1\n",
      "INFO:root:Round 1 complete. Val Stats: Loss=1.389565756980409, F1=0.10637014967723629, Time=2.88s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 2\n",
      "INFO:root:Training with 256 new samples\n",
      "INFO:root:Round 2 complete. Val Stats: Loss=1.3485149608013478, F1=0.13607678299913153, Time=10.12s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 3\n",
      "INFO:root:Training with 256 new samples\n",
      "INFO:root:Round 3 complete. Val Stats: Loss=1.3025882960634028, F1=0.20937402318126258, Time=16.87s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 4\n",
      "INFO:root:Training with 256 new samples\n",
      "INFO:root:Round 4 complete. Val Stats: Loss=1.3354038856130965, F1=0.2926970249099526, Time=24.01s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:\n",
      "--- Round 5\n",
      "INFO:root:Training with 256 new samples\n",
      "INFO:root:Round 5 complete. Val Stats: Loss=1.2030607924816457, F1=0.3041531736197598, Time=31.02s\n",
      "INFO:root:Sampled 256 new samples using RandomSampler\n",
      "INFO:root:Final Test set evaluation: Loss=1.213091655939567, F1=0.3048, Acc=0.4221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Metrics: F1=0.3048, Accuracy=0.4221, Loss=1.2131\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:03.150464Z",
     "start_time": "2025-08-29T14:12:03.145202Z"
    }
   },
   "cell_type": "code",
   "source": "al.save_experiment()",
   "id": "474893112e83c0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Experiment saved to experiments\\dummy_test_pipeline\\results_20250829_171203.json\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:44.012526Z",
     "start_time": "2025-08-29T14:12:44.009526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"experiments/dummy_test_pipeline/results_20250829_171203.json\", 'r') as f:\n",
    "    experiment_data = json.load(f)"
   ],
   "id": "94843c8c0ccce73f",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:44.405798Z",
     "start_time": "2025-08-29T14:12:44.402792Z"
    }
   },
   "cell_type": "code",
   "source": "print(experiment_data.keys())",
   "id": "b4221facf5bc88e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cfg', 'total_rounds', 'round_val_stats', 'final_pool_stats', 'final_test_stats'])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:44.771499Z",
     "start_time": "2025-08-29T14:12:44.766904Z"
    }
   },
   "cell_type": "code",
   "source": "experiment_data['cfg']",
   "id": "63253569c2715cd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'total_rounds': 5,\n",
       " 'initial_pool_size': 100,\n",
       " 'acquisition_batch_size': 256,\n",
       " 'sampler_class': 'RandomSampler',\n",
       " 'sampler_kwargs': {},\n",
       " 'strategy_class': 'FineTuneStrategy',\n",
       " 'strategy_kwargs': {},\n",
       " 'model_name_or_path': 'distilbert-base-uncased',\n",
       " 'num_labels': 4,\n",
       " 'tokenizer_kwargs': {'max_length': 128,\n",
       "  'padding': 'max_length',\n",
       "  'truncation': True,\n",
       "  'add_special_tokens': True,\n",
       "  'return_tensors': 'pt'},\n",
       " 'optimizer_class': 'Adam',\n",
       " 'optimizer_kwargs': {'lr': 0.001, 'weight_decay': 0.0001},\n",
       " 'criterion_class': 'CrossEntropyLoss',\n",
       " 'criterion_kwargs': {},\n",
       " 'scheduler_class': 'StepLR',\n",
       " 'scheduler_kwargs': {'step_size': 10, 'gamma': 0.1},\n",
       " 'device': 'cuda',\n",
       " 'epochs': 3,\n",
       " 'batch_size': 64,\n",
       " 'data': 'agnews',\n",
       " 'save_dir': 'experiments',\n",
       " 'experiment_name': 'dummy_test_pipeline'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:45.212975Z",
     "start_time": "2025-08-29T14:12:45.208773Z"
    }
   },
   "cell_type": "code",
   "source": "experiment_data['total_rounds']",
   "id": "2e45af560a7164f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:45.573394Z",
     "start_time": "2025-08-29T14:12:45.568777Z"
    }
   },
   "cell_type": "code",
   "source": "experiment_data['round_val_stats'][-1]",
   "id": "bd769b5d17ffa236",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_time': 31.023064613342285,\n",
       " 'avg_loss': 1.2513243467719466,\n",
       " 'epochs': 3,\n",
       " 'total_samples': 1124,\n",
       " 'new_samples': 256,\n",
       " 'loss': 1.2030607924816457,\n",
       " 'f1_score': 0.3041531736197598,\n",
       " 'accuracy': 0.42633333333333334,\n",
       " 'pool_stats': {'labeled_count': 1124,\n",
       "  'unlabeled_count': 106876,\n",
       "  'total_count': 108000}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:46.038133Z",
     "start_time": "2025-08-29T14:12:46.033998Z"
    }
   },
   "cell_type": "code",
   "source": "experiment_data[\"final_pool_stats\"]",
   "id": "1ae22899026cd37d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labeled_count': 1124, 'unlabeled_count': 106876, 'total_count': 108000}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:12:46.458543Z",
     "start_time": "2025-08-29T14:12:46.454409Z"
    }
   },
   "cell_type": "code",
   "source": "experiment_data[\"final_test_stats\"]",
   "id": "f3ff7f1ae7f58d9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.213091655939567,\n",
       " 'f1_score': 0.3047702822865449,\n",
       " 'accuracy': 0.42210526315789476}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T12:35:52.418759100Z",
     "start_time": "2025-08-28T21:03:15.306884Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "485a0c36ff2d07c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
